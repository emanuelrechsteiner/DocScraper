---
url: https://python.langchain.com/assets/js/a000346b.d877739d.js
scraped_at: 2025-05-25T18:06:39.638072
title: Untitled
---

```
"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[79749],{95283:(e,a,n)=>{n.r(a),n.d(a,{assets:()=>c,contentTitle:()=>s,default:()=>d,frontMatter:()=>r,metadata:()=>l,toc:()=>h});var t=n(74848),o=n(28453),i=n(63142);const r={custom_edit_url:"https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/sequence.ipynb",keywords:["Runnable","Runnables","RunnableSequence","LCEL","chain","chains","chaining"]},s="How to chain runnables",l={id:"how_to/sequence",title:"How to chain runnables",description:"This guide assumes familiarity with the following concepts:",source:"@site/docs/how_to/sequence.md",sourceDirName:"how_to",slug:"/how_to/sequence",permalink:"/docs/how_to/sequence",draft:!1,unlisted:!1,editUrl:"https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/sequence.ipynb",tags:[],version:"current",frontMatter:{custom_edit_url:"https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/sequence.ipynb",keywords:["Runnable","Runnables","RunnableSequence","LCEL","chain","chains","chaining"]},sidebar:"docs",previous:{title:"How to split text based on semantic similarity",permalink:"/docs/how_to/semantic-chunker"},next:{title:"How to save and load LangChain objects",permalink:"/docs/how_to/serialization"}},c={},h=[{value:"The pipe operator: <code>|</code>",id:"the-pipe-operator-",level:2},{value:"Coercion",id:"coercion",level:3},{value:"The <code>.pipe()</code> method",id:"the-pipe-method",level:2},{value:"Related",id:"related",level:2}];function p(e){const a={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(a.header,{children:(0,t.jsx)(a.h1,{id:"how-to-chain-runnables",children:"How to chain runnables"})}),"\n",(0,t.jsxs)(a.admonition,{title:"Prerequisites",type:"info",children:[(0,t.jsx)(a.p,{children:"This guide assumes familiarity with the following concepts:"}),(0,t.jsxs)(a.ul,{children:["\n",(0,t.jsx)(a.li,{children:(0,t.jsx)(a.a,{href:"/docs/concepts/lcel",children:"LangChain Expression Language (LCEL)"})}),"\n",(0,t.jsx)(a.li,{children:(0,t.jsx)(a.a,{href:"/docs/concepts/prompt_templates",children:"Prompt templates"})}),"\n",(0,t.jsx)(a.li,{children:(0,t.jsx)(a.a,{href:"/docs/concepts/chat_models",children:"Chat models"})}),"\n",(0,t.jsx)(a.li,{children:(0,t.jsx)(a.a,{href:"/docs/concepts/output_parsers",children:"Output parser"})}),"\n"]})]}),"\n",(0,t.jsxs)(a.p,{children:["One point about ",(0,t.jsx)(a.a,{href:"/docs/concepts/lcel",children:"LangChain Expression Language"}),' is that any two runnables can be "chained" together into sequences. The output of the previous runnable\'s ',(0,t.jsx)(a.code,{children:".invoke()"})," call is passed as input to the next runnable. This can be done using the pipe operator (",(0,t.jsx)(a.code,{children:"|"}),"), or the more explicit ",(0,t.jsx)(a.code,{children:".pipe()"})," method, which does the same thing."]}),"\n",(0,t.jsxs)(a.p,{children:["The resulting ",(0,t.jsx)(a.a,{href:"https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableSequence.html",children:(0,t.jsx)(a.code,{children:"RunnableSequence"})})," is itself a runnable, which means it can be invoked, streamed, or further chained just like any other runnable. Advantages of chaining runnables in this way are efficient streaming (the sequence will stream output as soon as it is available), and debugging and tracing with tools like ",(0,t.jsx)(a.a,{href:"/docs/how_to/debugging",children:"LangSmith"}),"."]}),"\n",(0,t.jsxs)(a.h2,{id:"the-pipe-operator-",children:["The pipe operator: ",(0,t.jsx)(a.code,{children:"|"})]}),"\n",(0,t.jsxs)(a.p,{children:["To show off how this works, let's go through an example. We'll walk through a common pattern in LangChain: using a ",(0,t.jsx)(a.a,{href:"/docs/how_to#prompt-templates",children:"prompt template"})," to format input into a ",(0,t.jsx)(a.a,{href:"/docs/how_to#chat-models",children:"chat model"}),", and finally converting the chat message output into a string with an ",(0,t.jsx)(a.a,{href:"/docs/how_to#output-parsers",children:"output parser"}),"."]}),"\n","\n",(0,t.jsx)(i.A,{customVarName:"model"}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-python",children:'\x3c!--IMPORTS:[{"imported": "StrOutputParser", "source": "langchain_core.output_parsers", "docs": "https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html", "title": "How to chain runnables"}, {"imported": "ChatPromptTemplate", "source": "langchain_core.prompts", "docs": "https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html", "title": "How to chain runnables"}]--\x3e\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\n\nprompt = ChatPromptTemplate.from_template("tell me a joke about {topic}")\n\nchain = prompt | model | StrOutputParser()\n'})}),"\n",(0,t.jsx)(a.p,{children:"Prompts and models are both runnable, and the output type from the prompt call is the same as the input type of the chat model, so we can chain them together. We can then invoke the resulting sequence like any other runnable:"}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-python",children:'chain.invoke({"topic": "bears"})\n'})}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-output",children:'"Here\'s a bear joke for you:\\n\\nWhy did the bear dissolve in water?\\nBecause it was a polar bear!"\n'})}),"\n",(0,t.jsx)(a.h3,{id:"coercion",children:"Coercion"}),"\n",(0,t.jsx)(a.p,{children:"We can even combine this chain with more runnables to create another chain. This may involve some input/output formatting using other types of runnables, depending on the required inputs and outputs of the chain components."}),"\n",(0,t.jsx)(a.p,{children:"For example, let's say we wanted to compose the joke generating chain with another chain that evaluates whether or not the generated joke was funny."}),"\n",(0,t.jsxs)(a.p,{children:["We would need to be careful with how we format the input into the next chain. In the below example, the dict in the chain is automatically parsed and converted into a ",(0,t.jsx)(a.a,{href:"/docs/how_to/parallel",children:(0,t.jsx)(a.code,{children:"RunnableParallel"})}),", which runs all of its values in parallel and returns a dict with the results."]}),"\n",(0,t.jsx)(a.p,{children:"This happens to be the same format the next prompt template expects. Here it is in action:"}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-python",children:'\x3c!--IMPORTS:[{"imported": "StrOutputParser", "source": "langchain_core.output_parsers", "docs": "https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html", "title": "How to chain runnables"}]--\x3e\nfrom langchain_core.output_parsers import StrOutputParser\n\nanalysis_prompt = ChatPromptTemplate.from_template("is this a funny joke? {joke}")\n\ncomposed_chain = {"joke": chain} | analysis_prompt | model | StrOutputParser()\n\ncomposed_chain.invoke({"topic": "bears"})\n'})}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-output",children:"'Haha, that\\'s a clever play on words! Using \"polar\" to imply the bear dissolved or became polar/polarized when put in water. Not the most hilarious joke ever, but it has a cute, groan-worthy pun that makes it mildly amusing. I appreciate a good pun or wordplay joke.'\n"})}),"\n",(0,t.jsx)(a.p,{children:"Functions will also be coerced into runnables, so you can add custom logic to your chains too. The below chain results in the same logical flow as before:"}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-python",children:'composed_chain_with_lambda = (\n  chain\n  | (lambda input: {"joke": input})\n  | analysis_prompt\n  | model\n  | StrOutputParser()\n)\n\ncomposed_chain_with_lambda.invoke({"topic": "beets"})\n'})}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-output",children:"\"Haha, that's a cute and punny joke! I like how it plays on the idea of beets blushing or turning red like someone blushing. Food puns can be quite amusing. While not a total knee-slapper, it's a light-hearted, groan-worthy dad joke that would make me chuckle and shake my head. Simple vegetable humor!\"\n"})}),"\n",(0,t.jsxs)(a.p,{children:["However, keep in mind that using functions like this may interfere with operations like streaming. See ",(0,t.jsx)(a.a,{href:"/docs/how_to/functions",children:"this section"})," for more information."]}),"\n",(0,t.jsxs)(a.h2,{id:"the-pipe-method",children:["The ",(0,t.jsx)(a.code,{children:".pipe()"})," method"]}),"\n",(0,t.jsxs)(a.p,{children:["We could also compose the same sequence using the ",(0,t.jsx)(a.code,{children:".pipe()"})," method. Here's what that looks like:"]}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-python",children:'\x3c!--IMPORTS:[{"imported": "RunnableParallel", "source": "langchain_core.runnables", "docs": "https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableParallel.html", "title": "How to chain runnables"}]--\x3e\nfrom langchain_core.runnables import RunnableParallel\n\ncomposed_chain_with_pipe = (\n  RunnableParallel({"joke": chain})\n  .pipe(analysis_prompt)\n  .pipe(model)\n  .pipe(StrOutputParser())\n)\n\ncomposed_chain_with_pipe.invoke({"topic": "battlestar galactica"})\n'})}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-output",children:'"I cannot reproduce any copyrighted material verbatim, but I can try to analyze the humor in the joke you provided without quoting it directly.\\n\\nThe joke plays on the idea that the Cylon raiders, who are the antagonists in the Battlestar Galactica universe, failed to locate the human survivors after attacking their home planets (the Twelve Colonies) due to using an outdated and poorly performing operating system (Windows Vista) for their targeting systems.\\n\\nThe humor stems from the juxtaposition of a futuristic science fiction setting with a relatable real-world frustration \u2013 the use of buggy, slow, or unreliable software or technology. It pokes fun at the perceived inadequacies of Windows Vista, which was widely criticized for its performance issues and other problems when it was released.\\n\\nBy attributing the Cylons\' failure to locate the humans to their use of Vista, the joke creates an amusing and unexpected connection between a fictional advanced race of robots and a familiar technological annoyance experienced by many people in the real world.\\n\\nOverall, the joke relies on incongruity and relatability to generate humor, but without reproducing any copyrighted material directly."\n'})}),"\n",(0,t.jsx)(a.p,{children:"Or the abbreviated:"}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-python",children:'composed_chain_with_pipe = RunnableParallel({"joke": chain}).pipe(\n  analysis_prompt, model, StrOutputParser()\n)\n'})}),"\n",(0,t.jsx)(a.h2,{id:"related",children:"Related"}),"\n",(0,t.jsxs)(a.ul,{children:["\n",(0,t.jsxs)(a.li,{children:[(0,t.jsx)(a.a,{href:"/docs/how_to/streaming/",children:"Streaming"}),": Check out the streaming guide to understand the streaming behavior of a chain"]}),"\n"]})]})}function d(e={}){const{wrapper:a}={...(0,o.R)(),...e.components};return a?(0,t.jsx)(a,{...e,children:(0,t.jsx)(p,{...e})}):p(e)}},63142:(e,a,n)=>{n.d(a,{A:()=>s,b:()=>r});var t=n(96540),o=n(21432),i=n(74848);const r=e=>{let{selectedOption:a,options:n,onSelect:o,modelType:r}=e;const[s,l]=t.useState(!1);t.useEffect((()=>{const e=e=>{s&&!e.target.closest(".dropdown")&&l(!1)};return document.addEventListener("click",e),()=>document.removeEventListener("click",e)}),[s]);const{text:c,link:h}=(()=>{switch(r){case"chat":default:return{text:"chat model",link:"/docs/integrations/chat/"};case"embeddings":return{text:"embeddings model",link:"/docs/integrations/text_embedding/"};case"vectorstore":return{text:"vector store",link:"/docs/integrations/vectorstores/"}}})();return(0,i.jsxs)("div",{style:{display:"flex",alignItems:"center",marginBottom:"1rem",gap:"0.75rem"},children:[(0,i.jsxs)("span",{style:{fontSize:"1rem",fontWeight:"500"},children:["Select ",(0,i.jsx)("a",{href:h,children:c}),":"]}),(0,i.jsxs)("div",{className:"dropdown "+(s?"dropdown--show":""),children:[(0,i.jsxs)("button",{className:"button button--secondary",onClick:()=>l(!s),style:{backgroundColor:"var(--ifm-background-color)",border:"1px solid var(--ifm-color-emphasis-300)",fontWeight:"normal",fontSize:"1rem",padding:"0.5rem 1rem",color:"var(--ifm-font-color-base)"},children:[a.label,(0,i.jsx)("span",{style:{marginLeft:"0.4rem",fontSize:"0.875rem"},children:"\u25be"})]}),(0,i.jsx)("div",{className:"dropdown__menu",style:{maxHeight:"210px",overflowY:"auto",overflowX:"hidden",marginBottom:0},children:n.map((e=>(0,i.jsx)("li",{children:(0,i.jsx)("a",{className:"dropdown__link "+(e.value===a.value?"dropdown__link--active":""),href:"#",onClick:a=>{a.preventDefault(),o(e.value),l(!1)},children:e.label})},e.value)))})]})]})};function s(e){const[a,n]=(0,t.useState)("openai"),{overrideParams:s,customVarName:l}=e,c=l??"model",h=[{value:"openai",label:"OpenAI",model:"gpt-4o-mini",apiKeyName:"OPENAI_API_KEY",packageName:"langchain[openai]"},{value:"anthropic",label:"Anthropic",model:"claude-3-5-sonnet-latest",apiKeyName:"ANTHROPIC_API_KEY",packageName:"langchain[anthropic]"},{value:"azure",label:"Azure",text:`from langchain_openai import AzureChatOpenAI\n\n${c} = AzureChatOpenAI(\n  azure_endpoint=os.environ["AZURE_OPENAI_ENDPOINT"],\n  azure_deployment=os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"],\n  openai_api_version=os.environ["AZURE_OPENAI_API_VERSION"],\n)`,apiKeyName:"AZURE_OPENAI_API_KEY",packageName:"langchain[openai]"},{value:"google_genai",label:"Google Gemini",model:"gemini-2.0-flash",apiKeyName:"GOOGLE_API_KEY",packageName:"langchain[google-genai]"},{value:"google_vertexai",label:"Google Vertex",model:"gemini-2.0-flash-001",apiKeyText:"# Ensure your VertexAI credentials are configured",packageName:"langchain[google-vertexai]"},{value:"bedrock_converse",label:"AWS",model:"anthropic.claude-3-5-sonnet-20240620-v1:0",apiKeyText:"# Ensure your AWS credentials are configured",packageName:"langchain[aws]"},{value:"groq",label:"Groq",model:"llama3-8b-8192",apiKeyName:"GROQ_API_KEY",packageName:"langchain[groq]"},{value:"cohere",label:"Cohere",model:"command-r-plus",apiKeyName:"COHERE_API_KEY",packageName:"langchain[cohere]"},{value:"nvidia",label:"NVIDIA",model:"meta/llama3-70b-instruct",apiKeyName:"NVIDIA_API_KEY",packageName:"langchain-nvidia-ai-endpoints"},{value:"fireworks",label:"Fireworks AI",model:"accounts/fireworks/models/llama-v3p1-70b-instruct",apiKeyName:"FIREWORKS_API_KEY",packageName:"langchain[fireworks]"},{value:"mistralai",label:"Mistral AI",model:"mistral-large-latest",apiKeyName:"MISTRAL_API_KEY",packageName:"langchain[mistralai]"},{value:"together",label:"Together AI",model:"mistralai/Mixtral-8x7B-Instruct-v0.1",apiKeyName:"TOGETHER_API_KEY",packageName:"langchain[together]"},{value:"ibm",label:"IBM watsonx",text:`from langchain_ibm import ChatWatsonx\n\n${c} = ChatWatsonx(\n  model_id="ibm/granite-34b-code-instruct", \n  url="https://us-south.ml.cloud.ibm.com", \n  project_id="<WATSONX PROJECT_ID>"\n)`,apiKeyName:"WATSONX_APIKEY",packageName:"langchain-ibm"},{value:"databricks",label:"Databricks",text:`from databricks_langchain import ChatDatabricks\n\nos.environ["DATABRICKS_HOST"] = "https://example.staging.cloud.databricks.com/serving-endpoints"\n\n${c} = ChatDatabricks(endpoint="databricks-meta-llama-3-1-70b-instruct")`,apiKeyName:"DATABRICKS_TOKEN",packageName:"databricks-langchain"},{value:"xai",label:"xAI",model:"grok-2",apiKeyName:"XAI_API_KEY",packageName:"langchain-xai"},{value:"perplexity",label:"Perplexity",model:"llama-3.1-sonar-small-128k-online",apiKeyName:"PPLX_API_KEY",packageName:"langchain-perplexity"}].map((e=>({...e,...s?.[e.value]}))),p=h.map((e=>({value:e.value,label:e.label}))),d=h.find((e=>e.value===a));let u="";d.apiKeyName?u=`import getpass\nimport os\n\nif not os.environ.get("${d.apiKeyName}"):\n os.environ["${d.apiKeyName}"] = getpass.getpass("Enter API key for ${d.label}: ")`:d.apiKeyText&&(u=d.apiKeyText);const m=d?.text||`from langchain.chat_models import init_chat_model\n\n${c} = init_chat_model("${d.model}", model_provider="${d.value}"${d?.kwargs?`, ${d.kwargs}`:""})`;return(0,i.jsxs)("div",{children:[(0,i.jsx)(r,{selectedOption:d,options:p,onSelect:n,modelType:"chat"}),(0,i.jsx)(o.A,{language:"bash",children:`pip install -qU "${d.packageName}"`}),(0,i.jsx)(o.A,{language:"python",children:u?u+"\n\n"+m:m})]})}}}]);
```


